{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1 - Computational Learning Course -  Apr-2023\n",
        "\n",
        "<br>Roei Zaady\t318747946</br>\n",
        "<br>Omer Yanai\t 024093866</br>"
      ],
      "metadata": {
        "id": "Fy2Q25WuZA0Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLL3S5BJXQOh"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S7yMMG2o3uuw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "###############\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils.estimator_checks import check_estimator\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "###############\n",
        "\n",
        "np.random.seed(1234) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xsMzDOVXU0v"
      },
      "source": [
        "## 1,2,3,4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UZKw0TD-3Tnm"
      },
      "outputs": [],
      "source": [
        "# 1 - Calculate entropy\n",
        "def compute_entropy(y):\n",
        "    if isinstance(y, int) == False:  # int means a single value, therefore there is no entropy\n",
        "        if y.shape[0] > 0:  # if array is not empty\n",
        "            p1 = y.sum() / y.shape[0]  # percentile of 1s\n",
        "            if (p1 > 0) & (p1 < 1):\n",
        "                return (-p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)).astype(float)\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 2 - Split dataset\n",
        "def split_dataset(X, node_indices, feature):\n",
        "    left_indices = list(np.array(node_indices)[X[node_indices, feature] == 1])\n",
        "    right_indices = list(np.array(node_indices)[X[node_indices, feature] == 0])\n",
        "    return left_indices, right_indices\n",
        "\n",
        "\n",
        "# 3 - calculate information gain\n",
        "def compute_information_gain(X, y, node_indices, feature):\n",
        "    sp_l, sp_r = split_dataset(X, node_indices, feature)\n",
        "    return compute_entropy(np.array(y[node_indices])) - (\n",
        "                len(sp_l) / len(node_indices) * compute_entropy(y[sp_l]) + len(sp_r) / len(\n",
        "            node_indices) * compute_entropy(y[sp_r]))\n",
        "\n",
        "\n",
        "# 4 - Get best split\n",
        "def get_best_split(X, y, node_indices):\n",
        "    if len(y[node_indices]) > 1:\n",
        "        temp_arr = []\n",
        "        for num in range(X.shape[1]):\n",
        "            temp_arr.append(compute_information_gain(X, y, node_indices, num))\n",
        "        if len(temp_arr) > 0:\n",
        "            return temp_arr.index(max(temp_arr))\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwjLsTQY-ml"
      },
      "source": [
        "## 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bv_ayvxo2wfc"
      },
      "outputs": [],
      "source": [
        "# 5 - Building a Tree\n",
        "class MyID3(BaseEstimator):\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        _estimator_type = 'classifier'\n",
        "\n",
        "\n",
        "    def build_node(self, X, y, node_indices):\n",
        "        self.right_node_ = None  # right child\n",
        "        self.left_node_ = None  # left child\n",
        "        self.values_ = None  # [num of zeros, num of ones]\n",
        "        self.entropy_ = None\n",
        "        self.feature_split_on_ = None\n",
        "        available_features = list(range(X.shape[1]))\n",
        "        self.entropy_ = compute_entropy(y[node_indices])\n",
        "        y1 = y[node_indices]\n",
        "        self.values_ = [y1.tolist().count(0), y1.tolist().count(1)]\n",
        "        # check if max depth have not been reached yet\n",
        "        not_max_depth_yet = -1\n",
        "        if self.max_depth is None:\n",
        "            not_max_depth_yet = 1\n",
        "            max_depth_for_child = None\n",
        "        elif self.max_depth > 0:\n",
        "            not_max_depth_yet = 1\n",
        "            max_depth_for_child = self.max_depth - 1\n",
        "\n",
        "        # build further branches\n",
        "        if not_max_depth_yet == 1:\n",
        "            if (self.values_[0] == 0) | (self.values_[1] == 0):\n",
        "                return self\n",
        "            self.feature_split_on_ = get_best_split(X, y, node_indices)\n",
        "            if compute_information_gain(X, y, node_indices, self.feature_split_on_) == 0:\n",
        "                return self\n",
        "            if X.shape[1] > 0:\n",
        "                available_features.remove(self.feature_split_on_)\n",
        "            if (self.values_[0] > 0) & (self.values_[1] > 0):\n",
        "                left_indices, right_indices = split_dataset(X, node_indices, self.feature_split_on_)\n",
        "                if(len(left_indices) > 0) & (len(right_indices) > 0):\n",
        "                  self.left_node_ = MyID3(max_depth=max_depth_for_child)\n",
        "                  self.left_node_.build_node(X[:, available_features], y, left_indices)\n",
        "                  self.right_node_ = MyID3(max_depth=max_depth_for_child)\n",
        "                  self.right_node_.build_node(X[:, available_features], y, right_indices)\n",
        "            return self\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y.ravel())\n",
        "        self.classes_ = unique_labels(y)\n",
        "        self.build_node(X, y, list(range(X.shape[0])))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "      X_copy = list(np.copy(X))\n",
        "      feature = -1\n",
        "      temp = self\n",
        "      while temp.right_node_ is not None:\n",
        "          feature = temp.feature_split_on_\n",
        "          if X_copy[temp.feature_split_on_] == 0:\n",
        "              temp = temp.right_node_\n",
        "          else:\n",
        "              temp = temp.left_node_\n",
        "          X_copy.pop(feature)\n",
        "      if temp.values_[0] >= temp.values_[1]:\n",
        "        outcome = temp.values_[0] / (temp.values_[0] + temp.values_[1])\n",
        "        return np.array([[outcome], [1 - outcome]]).reshape(2,1)\n",
        "      else:\n",
        "        outcome = temp.values_[1] / (temp.values_[0] + temp.values_[1])\n",
        "      return np.array([[1 - outcome], [outcome]]).reshape(2,1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self)\n",
        "        if X.ndim > 1:\n",
        "          X = check_array(X)\n",
        "        y = []\n",
        "        if X.ndim == 1:\n",
        "          temp = list(self.predict_proba(X))\n",
        "          y = temp.index(max(temp))\n",
        "        else:\n",
        "          for row in range(len(X)):\n",
        "            temp = self.predict_proba(X[row, :])\n",
        "            y.append(list(temp).index(max(temp)))\n",
        "        return y\n",
        "\n",
        "    def tree_mapper(self): #get num of leaves in tree\n",
        "      if self.right_node_ is None:\n",
        "        return 1\n",
        "      return 0 + self.right_node_.tree_mapper() + self.left_node_.tree_mapper()\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "      return {'max_depth': self.max_depth}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyDR8z1bZHZS"
      },
      "source": [
        "## 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rMgnxeL-Iw5P"
      },
      "outputs": [],
      "source": [
        "# 6 - Bagging\n",
        "class MyBaggingID3(BaseEstimator):\n",
        "    def __init__(self, n_estimators=3, max_samples=1, max_features=1, max_depth=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees_array_ = []\n",
        "        self.features_ = []\n",
        "        for time in range(self.n_estimators):\n",
        "            indices = np.random.choice(X.shape[0], size=int(X.shape[0] * self.max_samples), replace=True)\n",
        "            X_with_random = X[indices, :]\n",
        "            y1 = np.array(y)[indices]\n",
        "\n",
        "            if self.max_features < 1:\n",
        "                num_of_features = np.random.choice(X.shape[1], int(X.shape[1] * self.max_features), replace=False)\n",
        "                X_with_random = X_with_random[:, num_of_features]\n",
        "                self.features_.append(num_of_features)\n",
        "            else:\n",
        "                num_of_features = range(X.shape[1])\n",
        "                self.features_.append(num_of_features)\n",
        "\n",
        "            temp_tree = MyID3(max_depth=self.max_depth)\n",
        "            temp_tree.fit(X_with_random, y1)\n",
        "            self.trees_array_.append(temp_tree)\n",
        "      \n",
        "    def predict_proba(self, X):\n",
        "        pred_of_individual_tree = []\n",
        "        scores = []\n",
        "        zeros = 0\n",
        "        ones = 0\n",
        "        if np.array(X).ndim == 1:\n",
        "            times = range(1)\n",
        "        else:\n",
        "            times = range(X.shape[0])\n",
        "        for row in times:\n",
        "            zeros, ones = 0, 0\n",
        "            for i in range(self.n_estimators):\n",
        "                features = self.features_[i]\n",
        "                if np.array(X).ndim == 1:\n",
        "                    zero_, one_ = self.trees_array_[i].predict_proba(X[features])\n",
        "                else:\n",
        "                    zero_, one_ = self.trees_array_[i].predict_proba(X[row, features])\n",
        "                zeros = zeros + zero_\n",
        "                ones = ones + one_\n",
        "            scores.append([zeros / self.n_estimators, ones / self.n_estimators])\n",
        "        return scores\n",
        "\n",
        "    def predict(self, X):\n",
        "        temp = self.predict_proba(X)\n",
        "        score_array = []\n",
        "        for cell in range(len(temp)):\n",
        "          score_array.append(temp[cell].index(max(temp[cell])))\n",
        "        if len(score_array)==1:\n",
        "          return max(score_array)\n",
        "        return score_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSyPWu8XZMaX"
      },
      "source": [
        "## 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU        # install Weights&Biasses for data collection and reporting.\n",
        "\n",
        "import wandb\n",
        "wandb.login()  #349ecabe67af04ba71e0596c9223822e18cd88d7     #aa5982516882260d70edad6ff1b9a5d9c4e348b9    Token for login."
      ],
      "metadata": {
        "id": "xPW8VMfB3qjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jVcz4_D9W4Hn"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset):\n",
        "  if dataset=='mushroom':   ### https://www.kaggle.com/datasets/uciml/mushroom-classification\n",
        "    df = pd.read_csv('https://drive.google.com/uc?id=16iZDKB6GGc6_6UAoyf7Ng8L6Yn6FofQW&export=download')\n",
        "    X = df.drop(['class'],axis=1)\n",
        "    y = df['class']\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(y).reshape(-1,1)\n",
        "    X = np.array(pd.get_dummies(X, columns=X.columns))\n",
        "\n",
        "  if dataset == 'haberman':    ### from UCI\n",
        "    df =  pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\",names=['A','B','C','D'])\n",
        "    X = df.drop(['D'],axis=1)\n",
        "    y = df['D']\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(y).reshape(-1,1)    \n",
        "    X = np.array(pd.get_dummies(df, columns=df.columns))\n",
        "\n",
        "  if dataset == 'breast-cancer-wisconsin':    ## from UCI\n",
        "    df =  pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",names=['id','Clump','Size','Shape','Adhesion','Epithelial','Nuclei','Chromatin','Normal','Mitoses','Class'])\n",
        "    y = df['Class'] \n",
        "    df = df.drop(['Class', 'id'],axis=1)\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(y).reshape(-1,1)    \n",
        "    X = np.array(pd.get_dummies(df, columns=df.columns))\n",
        "\n",
        "  if dataset=='monk':     ###  from UCI\n",
        "      df =  pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-2.test\",delimiter=' ',names=['label','a1','a2','a3','a4','a5','a6','id'])\n",
        "      y= df['label']\n",
        "      df = df.drop(['label','id'],axis=1)\n",
        "      y=y.values.reshape(-1,1)\n",
        "      X = np.array(pd.get_dummies(df, columns=df.columns))\n",
        "\n",
        "  if dataset=='tic-tac-toe':     ###  from UCI\n",
        "    df =  pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data\",names=['1','2','3','4','5','6','7','8','9','label'])\n",
        "    y= df['label']\n",
        "    df = df.drop(['label'],axis=1)\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(y).reshape(-1,1)    \n",
        "    X = np.array(pd.get_dummies(df, columns=df.columns))\n",
        "\n",
        "  return X,y.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLoqO5NlfZQZ"
      },
      "source": [
        "## 7.2 - Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load config fils with all combinations of dataset, classifier, hyper-params\n",
        "df_runs = pd.read_csv('https://drive.google.com/uc?id=1zHPqGjzDbLzjaswQA80OFHUsKRhqR3LH&export=download',encoding='UTF-8')\n",
        "\n",
        "#for partial run\n",
        "# df_for_table = df_runs.iloc[[5,15,27,37,41,46,53,58,9,19], :]     # partial list for report.\n",
        "# df_for_table\n",
        "\n",
        "#for full run\n",
        "df_for_table = df_runs"
      ],
      "metadata": {
        "id": "6nrYTeXXcwuZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def run_and_log_classifier(project,name,params):\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "scoring = {'accuracy' : make_scorer(accuracy_score),\n",
        "           'precision' : make_scorer(precision_score),\n",
        "           'recall' : make_scorer(recall_score),\n",
        "           'f1_score' : make_scorer(f1_score),\n",
        "           'auc-roc': make_scorer(roc_auc_score)}\n",
        "results=[]\n",
        "kfold = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2)\n",
        "for run in range(df_for_table.shape[0]):\n",
        "  if df_for_table.iloc[run]['depth'] == 'None':\n",
        "    depth = None\n",
        "  else:\n",
        "    depth = int(df_for_table.iloc[run]['depth'])\n",
        "\n",
        "  X,y = prepare_dataset(df_for_table.iloc[run]['dataset'])\n",
        "\n",
        "  if df_for_table.iloc[run]['classifier']=='MyBaggingID3':\n",
        "    clf=MyBaggingID3(n_estimators = df_for_table.iloc[run]['n_estimators'], max_depth = depth,\n",
        "                     max_features=df_for_table.iloc[run]['max_features'],max_samples=df_for_table.iloc[run]['max_samples'])\n",
        "  else:\n",
        "    clf=BaggingClassifier(n_estimators = df_for_table.iloc[run]['n_estimators'], \n",
        "                          max_features=df_for_table.iloc[run]['max_features'],max_samples=df_for_table.iloc[run]['max_samples'], \n",
        "                          estimator= DecisionTreeClassifier(criterion=\"entropy\", max_depth = depth))\n",
        "  \n",
        "  res = cross_validate(estimator=clf,\n",
        "                              X=np.array(X),\n",
        "                              y=y,\n",
        "                              cv=kfold,\n",
        "                              scoring=scoring, \n",
        "                              return_train_score=True,\n",
        "                              error_score=\"raise\")\n",
        "  # for running mean values only\n",
        "  # dic = pd.DataFrame(res).mean().to_dict()\n",
        "  # tag = [str(x) for x in list(df_for_table.iloc[run,:])]\n",
        "  # dic.update(dict(zip(df_for_table.columns,tag)))\n",
        "  # run_name=f'{df_for_table.iloc[run][\"dataset\"]}_{df_for_table.iloc[run][\"classifier\"]}'  \n",
        "  # print(run_name)\n",
        "  # wandb_run = wandb.init(project=\"CL_EX1\", name=run_name)\n",
        "  # wandb_run.log(dic)\n",
        "\n",
        "  #for full run\n",
        "  dic = pd.DataFrame(res).to_dict()\n",
        "  tag = [str(x) for x in list(df_for_table.iloc[run,:])]\n",
        "  run_name=f'{df_for_table.iloc[run][\"dataset\"]}_{df_for_table.iloc[run][\"classifier\"]}_{run}'  \n",
        "  print(run_name)\n",
        "  config = dict(zip(df_for_table.columns,tag))\n",
        "  wandb_run = wandb.init(project=\"CL_EX1_1\", name=run_name,config=config)\n",
        "  for key in res.keys():\n",
        "     for r in range(len(res[key])):\n",
        "          wandb_run.log({key:res[key][r]})\n",
        "  wandb.finish"
      ],
      "metadata": {
        "id": "wHTcr8aBTV4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## get shape of each dataset\n",
        "# for ds in ['tic-tac-toe','breast-cancer-wisconsin','mushroom','haberman','monk']:\n",
        "#   x,y=prepare_dataset(ds)\n",
        "#   print(ds,x.shape)"
      ],
      "metadata": {
        "id": "b8rQ_CxlhpAa"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KLL3S5BJXQOh",
        "-xsMzDOVXU0v",
        "oiwjLsTQY-ml",
        "wyDR8z1bZHZS",
        "wSyPWu8XZMaX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}